# ğŸ¯ Indonesian LLM Fine-tune

Kit untuk melakukan **fine-tuning** model LLM Bahasa Indonesia menggunakan teknik **LoRA (Low-Rank Adaptation)**. Dikembangkan oleh **Syahril Haryono** sebagai kelanjutan dari Indonesian LLM Starter.

> ğŸ’¡ **Repo ini adalah lanjutan dari [Indonesian LLM Starter](https://github.com/syhrlhyn834/Indonesian-LLM-Starter) â€” pastikan kamu sudah punya model hasil pre-training sebelum fine-tuning di sini.**

---

## ğŸ”— Ekosistem Aibys

| Repo | Fungsi |
|---|---|
| ğŸ“¦ [Aibys Data Collector](https://github.com/syhrlhyn834/Aibys-Data-Collector) | Kumpulkan & siapkan dataset untuk training |
| ğŸ—ï¸ [Indonesian LLM Starter](https://github.com/syhrlhyn834/Indonesian-LLM-Starter) | Pre-training LLM dari scratch |
| ğŸ¯ **Indonesian LLM Fine-tune** (repo ini) | Fine-tuning model hasil pre-training dengan LoRA |

**Alur lengkap:**
```
Aibys Data Collector    â†’    Indonesian LLM Starter    â†’    Indonesian LLM Fine-tune
(kumpul & siap data)         (pre-train model)               (fine-tune jadi assistant)
        â†“                            â†“                                  â†“
  train_shuffled.txt    â†’      aibys_final.pt           â†’        model siap chat
```

---

## ğŸ¯ Untuk Apa Repo Ini?

Model hasil pre-training dari [Indonesian LLM Starter](https://github.com/syhrlhyn834/Indonesian-LLM-Starter) sudah bisa generate teks, tapi belum bisa diajak ngobrol seperti assistant. Repo ini mengubahnya jadi model yang bisa **menjawab pertanyaan dan mengikuti instruksi** â€” dengan cara yang efisien menggunakan LoRA.

---

## âœ¨ Kenapa LoRA?

Fine-tuning model 500M parameter secara penuh butuh VRAM yang besar dan waktu lama. LoRA menyelesaikan ini dengan cara:

- **Freeze** semua parameter base model (tidak dilatih ulang)
- **Tambah** matriks kecil A dan B di layer attention
- **Latih** hanya matriks kecil tersebut (~1-2% dari total parameter)
- **Hasilnya** hampir sama bagusnya dengan full fine-tuning, tapi jauh lebih cepat dan hemat VRAM

```
Full fine-tuning  : ~500M params dilatih  âŒ berat
LoRA fine-tuning  : ~5-10M params dilatih âœ… ringan
```

---

## ğŸ“ Cara Kerja LoRA

LoRA menyisipkan matriks tambahan ke dalam layer attention:

```
# Sebelum LoRA
output = W_base(x)

# Sesudah LoRA
output = W_base(x) + (A @ B)(x) * scaling

# Dimana:
# W_base  = frozen, tidak berubah
# A, B    = matriks kecil yang dilatih (rank=16)
# scaling = alpha / rank
```

LoRA di-inject ke 4 projection layer di setiap attention block:
- `q_proj` â€” Query projection
- `k_proj` â€” Key projection
- `v_proj` â€” Value projection
- `out_proj` â€” Output projection

---

## ğŸ—‚ï¸ Struktur Project

```
aibys-finetune/
â”‚
â”œâ”€â”€ finetune_lora.py      # ğŸš€  Entry point â€” jalankan ini untuk fine-tuning
â”œâ”€â”€ merge_lora.py         # ğŸ”€  Merge LoRA weights ke base model jadi satu file
â”œâ”€â”€ chat_lora.py          # ğŸ’¬  Chat interaktif dengan model LoRA (tanpa merge)
â”œâ”€â”€ test_finetuned.py     # ğŸ§ª  Test model hasil fine-tune (chat / batch test)
â”‚
â”œâ”€â”€ model/                # Arsitektur model (sama seperti Indonesian LLM Starter)
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ aibys.py
â”‚   â”œâ”€â”€ block.py
â”‚   â”œâ”€â”€ attention.py
â”‚   â”œâ”€â”€ ffn.py
â”‚   â”œâ”€â”€ rmsnorm.py
â”‚   â”œâ”€â”€ rope.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ checkpoints/          # Output fine-tuning (tidak di-commit)
â”‚   â”œâ”€â”€ best_lora.pt      # Checkpoint dengan val loss terbaik
â”‚   â”œâ”€â”€ final_lora.pt     # Checkpoint step terakhir
â”‚   â””â”€â”€ final_lora_merged.pt  # Hasil merge (siap deploy)
â”‚
â”œâ”€â”€ base_model.pt         # Base model dari pre-training (tidak di-commit)
â””â”€â”€ tokenizer.model       # Tokenizer SentencePiece (tidak di-commit)
```

---

## âš™ï¸ Penjelasan Tiap File

### `finetune_lora.py` â€” Fine-tuning Utama
Script utama yang:
1. Load base model hasil pre-training
2. Inject LoRA ke layer attention
3. Freeze semua parameter kecuali LoRA
4. Load dataset `cahya/instructions_indonesian` dari HuggingFace
5. Fine-tune dengan training loop lengkap (mixed precision, LR scheduler, checkpoint)
6. Otomatis ganti nama AI lain (ChatGPT, Claude, Gemini, dll) jadi "Aibys" di data training

**LoRA Config default:**
```python
lora_rank = 16       # Rank matriks A dan B
lora_alpha = 32      # Scaling factor
lora_dropout = 0.05  # Dropout pada LoRA
```

### `merge_lora.py` â€” Gabungkan LoRA ke Base Model
Setelah fine-tuning, jalankan ini untuk menggabungkan LoRA weights ke base model:
```
W_final = W_base + (A @ B) * (alpha / rank)
```
Hasilnya satu file `.pt` yang bisa dipakai langsung tanpa perlu LoRA lagi.

### `chat_lora.py` â€” Chat Interaktif (dengan LoRA terpisah)
Load base model + LoRA weights secara terpisah dan langsung bisa chat. Cocok untuk quick test sebelum merge.

Format prompt yang dipakai:
```
Pengguna: {pertanyaan kamu}
Aibys: {jawaban model}
```

### `test_finetuned.py` â€” Test Model Hasil Merge
Test model yang sudah di-merge dengan dua mode:
- **`--mode chat`** â€” Chat interaktif bebas
- **`--mode test`** â€” Batch test dengan 6 pertanyaan preset
- **`--mode both`** â€” Keduanya

---

## ğŸš€ Cara Pakai dari Awal

### Prerequisite
- Sudah punya model hasil pre-training dari [Indonesian LLM Starter](https://github.com/syhrlhyn834/Indonesian-LLM-Starter)
- File `aibys_final.pt` (atau checkpoint terakhir) dari pre-training
- File `tokenizer/aibys.model` dari pre-training

### Step 1 â€” Clone & Install
```bash
git clone https://github.com/syhrlhyn834/indonesian-llm-finetune.git
cd indonesian-llm-finetune
pip install -r requirements.txt
```

### Step 2 â€” Siapkan File yang Dibutuhkan
Copy file dari hasil pre-training ke folder ini:
```bash
# Copy base model
cp ../Indonesian-LLM-Starter/checkpoints/aibys_final.pt ./base_model.pt

# Copy tokenizer
cp ../Indonesian-LLM-Starter/tokenizer/aibys.model ./tokenizer.model
```

### Step 3 â€” Fine-tuning
```bash
python finetune_lora.py
```

Training akan otomatis:
- Download dataset `cahya/instructions_indonesian` dari HuggingFace
- Log progress setiap step
- Evaluasi val loss setiap 250 steps
- Simpan checkpoint terbaik ke `checkpoints/best_lora.pt`
- Simpan checkpoint final ke `checkpoints/final_lora.pt`

### Step 4 â€” Merge LoRA ke Base Model
```bash
python merge_lora.py \
  --base base_model.pt \
  --lora checkpoints/best_lora.pt \
  --output checkpoints/final_lora_merged.pt
```

### Step 5 â€” Chat dengan Model
```bash
# Chat dengan model yang sudah di-merge (rekomendasi)
python test_finetuned.py --checkpoint checkpoints/final_lora_merged.pt --mode chat

# Atau chat langsung dengan LoRA (tanpa merge)
python chat_lora.py
```

### Step 6 â€” Batch Test
```bash
python test_finetuned.py --checkpoint checkpoints/final_lora_merged.pt --mode both
```

---

## ğŸ”§ Konfigurasi

Edit `FineTuneConfig` di `finetune_lora.py`:

```python
class FineTuneConfig:
    # LoRA
    lora_rank = 16          # Lebih besar = lebih ekspresif, lebih berat
    lora_alpha = 32         # Biasanya 2x rank
    lora_dropout = 0.05

    # Training
    batch_size = 4
    grad_accum_steps = 4    # Effective batch = 4 x 4 = 16
    learning_rate = 3e-4
    max_steps = 5000        # 5K steps cukup untuk fine-tune
    warmup_steps = 100

    # Dataset
    max_samples = None      # None = pakai semua data
    context_length = 512
```

**Tips penyesuaian GPU:**

| VRAM | `batch_size` | `grad_accum_steps` | `lora_rank` |
|---|---|---|---|
| 16GB | 8 | 2 | 16 |
| 8GB | 4 | 4 | 16 |
| 6GB | 2 | 8 | 8 |

---

## ğŸ“¦ Dependencies

```
torch>=2.1.0
sentencepiece>=0.1.99
datasets>=2.14.0
numpy>=1.24.0
tqdm>=4.66.0
```

```bash
pip install -r requirements.txt
```

---

## ğŸ—ºï¸ Roadmap

- [x] LoRA injection ke attention layers
- [x] Freeze base model, latih hanya LoRA
- [x] Dataset instruksi Bahasa Indonesia
- [x] Auto-replace nama AI lain jadi Aibys
- [x] Merge LoRA ke base model
- [x] Chat interaktif
- [x] Batch testing
- [ ] Support multi-turn conversation
- [ ] DPO / RLHF alignment
- [ ] Export ke GGUF setelah merge
- [ ] Support dataset custom (bawa data sendiri)

---

## ğŸ‘¤ Author

**Syahril Haryono** â€” Developer independen asal Indonesia.

---

## ğŸ“„ License

Apache 2.0 â€” bebas digunakan, dimodifikasi, dan didistribusikan dengan atribusi.

---

*Lanjutan dari [Indonesian LLM Starter](https://github.com/syhrlhyn834/Indonesian-LLM-Starter) â€” bangun AI assistant Bahasa Indonesia kamu sendiri. ğŸš€*
